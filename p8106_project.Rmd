---
title: "Predicting Diagnosis of Liver Disease "
output: pdf_document
---

```{r}
# Load libraries 
library(tidyverse) 
library(dplyr)
library(ISLR) 
library(janitor) 
library(AppliedPredictiveModeling) 
library(caret) 
library(corrplot) 
library(pROC) 
library(MASS) 
library(readxl)
library(glmnet)
library(mlbench)
library(pdp)
library(vip)
library(klaR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(DALEX)
library(gbm)
library(ROCR)

```

# Import data
```{r}
liver_df = read_excel("./data/liver.xlsx") %>%
  mutate(outcome = ifelse(is_patient == 1, "yes", "no"), outcome = as.factor(outcome)) %>%
  dplyr::select(-is_patient) %>%
  clean_names %>%
  rename(
    aspartate_aminotransferase = sgpt, 
    alamine_aminotransferase = sgot, 
    albumin_globulin_ratio = ag_ratio, 
    alkaline_phosphate = alkphos) %>%
  drop_na 

liver_df$gender=factor(x=liver_df$gender,levels = c('Female','Male'),labels=c(0, 1))
liver_df$gender = as.double(liver_df$gender) 
# female = '1', male = '2'
```


# Exploratory Data Analysis
```{r}
# Feature plots 
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df[, 1:10], 
            y = liver_df$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# dataset with all the log-transformed predictor variables 
liver_df1 =
  liver_df %>%
  mutate(logtot_bilirubin = log(tot_bilirubin) +1,
         logdirect_bilirubin = log(direct_bilirubin) +1,
         logtot_proteins = log(tot_proteins) +1,
         logalbumin = log(albumin) +1, 
         loggender = log(gender) +1,
         logalbumin_globulin_ratio = log(albumin_globulin_ratio) +1,
         logage = log(age) +1, 
         logaspartate_aminotransferase = log(aspartate_aminotransferase) +1,
         logalamine_transferase = log(alamine_aminotransferase ) +1,
         logalkaline_phosphate = log(alkaline_phosphate) +1) %>% 
         dplyr::select(logage, loggender, logtot_bilirubin, logdirect_bilirubin, logtot_proteins,
                       logalbumin, logalbumin_globulin_ratio, logaspartate_aminotransferase,
                       logalamine_transferase, logalkaline_phosphate, outcome)
  

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df1[, 1:10], 
            y = liver_df1$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# Correlation plot
corrplot(cor(liver_df[,-11]), tl.srt = 45, order = 'hclust', type = 'upper')


table(liver_df$outcome)
```

# Data Partition
```{r}
#Create Training and Test Datasets
set.seed(10)
dim(liver_df1)
rowTrain <- createDataPartition(y = liver_df1$outcome,
                                p = 0.8,
                                list = FALSE)

liver.train = liver_df1[rowTrain, ]
liver.test = liver_df1[-rowTrain, ]
```



# Logistic Regression 
```{r}
#Fit Logistic Regression Model with all predictors 
glm.fit <- glm(outcome ~ ., data = liver_df1,
               subset = rowTrain,
               family = binomial(link = "logit"))
summary(glm.fit)

# Confusion Matrix 
test.pred.prob <- predict(glm.fit, newdata = liver_df1[-rowTrain,], type = "response")
test.pred <- rep("no", length(test.pred.prob)) 
test.pred[test.pred.prob > 0.5] <- "yes"
confusionMatrix(data = as.factor(test.pred), reference = liver_df1$outcome[-rowTrain],
                positive = "yes")
# ROC Curve
roc.glm <- roc(liver_df1$outcome[-rowTrain], test.pred.prob) 
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.glm), col = 4, add = TRUE)

# Fit a logistic regression with CARET 
set.seed(10)
ctrl1 <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
model.glm <- train(x = liver_df1[rowTrain,1:10],
                   y = liver_df1$outcome[rowTrain],
                   method = "glm",
                   preProcess = c("center", "scale"), 
                   metric = "ROC",
                   trControl = ctrl1)
```



# Regularized logistic regression
```{r}
ctrl2 <- trainControl(method = "cv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

set.seed(10)
glmnGrid <- expand.grid(.alpha = seq(0,1,length =6),
                       .lambda = exp(seq(-6,0,length =20)))
model.glmn <- train(x=liver_df1[rowTrain,1:10],
                   y=liver_df1$outcome[rowTrain],
                   method ="glmnet",
                   #preProcess = c("center", "scale"), 
                   tuneGrid =glmnGrid,
                   metric ="ROC",
                   trControl =ctrl2)


ggplot(model.glmn,xTrans = function(x)log(x), highlight = TRUE)

max(model.glmn$result$ROC)
model.glmn$bestTune # alpha of 0 indicates a ridge regression. 

coef(model.glmn$finalModel, s = model.glmn$bestTune$lambda)
```


# MARS
```{r}
set.seed(10)
model.mars <- train(x = liver_df1[rowTrain,1:10],
                    y = liver_df1$outcome[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, nprune = 2:20), 
                    metric = "ROC",
                    trControl = ctrl2)

plot(model.mars)
coef(model.mars$finalModel)
vip(model.mars$finalModel)

```
# KNN
```{r}
set.seed(10)

model.knn <- train(x = liver_df1[rowTrain,1:10],
                   y = liver_df1$outcome[rowTrain],
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1, 200, by = 5)),  
                   trControl = ctrl1)

liver_df1$outcome[-rowTrain]
model.knn$bestTune
pred_knn = predict(model.knn, newdata = liver.test, type = 'prob')
roc_knn <- roc(liver_df1$outcome[-rowTrain], pred_knn[,2])
plot.roc(roc_knn, legacy.axes = TRUE, print.auc = TRUE)
```

# LDA
```{r}
set.seed(10)
lda.fit <- lda(outcome~., 
               data = liver.train)

set.seed(10)
model.lda <- train(x = liver_df1[rowTrain,1:10], 
                   y = liver_df1$outcome[rowTrain],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl1)
lda.pred <- predict(lda.fit, newdata = liver.test)
roc.lda <- roc(liver_df1[-rowTrain,]$outcome, lda.pred$posterior[,2])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```

# QDA
```{r}
set.seed(10)
qda.fit <- qda(outcome~., 
               data = liver.train)

set.seed(10)
model.qda <- train(x = liver_df1[rowTrain,1:10], 
                   y = liver_df1$outcome[rowTrain],
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl1)
qda.pred <- predict(qda.fit, newdata = liver.test)
roc.qda <- roc(liver_df1[-rowTrain,]$outcome, qda.pred$posterior[,2])
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)


auc <- c(roc.lda$auc[1], roc.qda$auc[1])
```


# Naive Bayes
```{r}
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE), 
                      fL = 1,
                      adjust = seq(.2, 5, by = .2))
model.nb <- train(x = liver_df1[rowTrain,1:10], 
                  y = liver_df1$outcome[rowTrain],
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl1)
plot(model.nb)
```



# CLASSIFICATION TREE
```{r}
set.seed(1)
tree1 = rpart(formula = outcome ~ ., data = liver_df1, subset = rowTrain, 
              control = rpart.control(cp = 0))
rpart.plot(tree1)
summary(tree1)
```

# RANDOM FORESTS
```{r}
ctrl3 <- trainControl(method = "cv", summaryFunction = twoClassSummary, classProbs = T)

model.rf2 <- train(outcome~., data = liver_df1, subset = rowTrain, method = "rpart", 
                   trControl = ctrl3, metric = "ROC")
RF.pred <- predict(model.rf2, newdata = liver_df1[-rowTrain, ])

confu_dtree <- confusionMatrix(RF.pred, liver_df1[-rowTrain, ]$outcome)
confu_dtree


set.seed(1)
rf2.final.per <- ranger(outcome ~ . , 
                        data = liver_df1[rowTrain,],
                        mtry = model.rf2$bestTune[[1]], 
                        splitrule = "gini",
                        min.node.size = model.rf2$bestTune[[3]],
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))

```

# SVM
```{r}
#linear
ctrl4 <-trainControl(method = "cv")
set.seed(10)
svm.linear.model <- train(
  outcome ~., data = liver.train, method = "svmLinear",
  trControl = ctrl4,
  #preProcess = c("center", "scale"),
  tuneGrid =data.frame(C =exp(seq(-10,1,len=20))),
  )

plot(svm.linear.model, highlight = TRUE, xTrans = log)
svm.linear.model$bestTune
svm.linear.model$finalModel
```

```{r}
ctrl4 <- trainControl(method = "cv", classProbs = T)
set.seed(10)
svmr.grid <- expand.grid(C = exp(seq(-10,1,len = 20)),
                         sigma = exp(seq(-8,0,len = 10)))
svm.radial.model <- train(outcome ~ . , liver_df1, 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  trControl = ctrl4,
                  prob.model = TRUE,
                  tuneGrid = svmr.grid, metric = "ROC")
plot(svm.linear2.model, highlight = TRUE, xTrans = log)
svm.linear2.model$bestTune
svm.linear2.model$finalModel
```

```{r}
svmr.grid <- expand.grid(C = exp(seq(-1, 4,len = 10)), sigma = exp(seq(-10,0,len = 10)))

# tunes over both cost and sigma
set.seed(10)


svm.radial.model <- train(outcome ~ . , liver_df1, 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  trControl = ctrl4, prob.model = T)

plot(svm.radial.model, highlight = TRUE)
svm.radial.model$bestTune
svm.radial.model$finalModel

SVM.pred = predict(svm.radial.model, newdata = liver_df1[-rowTrain, ]) 
confusionMatrix(data = SVM.pred, reference = liver_df1[-rowTrain, ]$outcome)
```

```{r}
x_train <- as.matrix(liver.train[,1:10])

explainer_svm <- explain(svm.radial.model,
                         label = "svmr",
                         data = x_train,
                         y = as.numeric(liver_df1$outcome[rowTrain] == "yes"),
                         verbose = FALSE)
vi_svm <- model_parts(explainer_svm)
plot(vi_svm)
```

# Boosting
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

ctrl.boost <- trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
set.seed(10)
gbmA.model <- train(outcome ~ . ,
                  data = liver.train,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl.boost,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.model, highlight = TRUE)

gbmImp <- varImp(gbmA.model, scale = FALSE)
plot(gbmImp)
```

# Let's select the Final Model by comparing each model's mean cross-validation AUC 
```{r}
res <- resamples(list(MARS = model.mars, 
                      LDA = model.lda, 
                      QDA = model.qda, 
                      GLM = model.glm, 
                      KNN = model.knn,
                      GBM = gbmA.model,
                      NB = model.nb, 
                      RF = model.rf2)) 
summary(res) # SVM not included
bwplot(res, metric = "ROC")

resamps_svm <- resamples(list(svmr = svm.radial.model, svml = svm.linear.model))
summary(resamps_svm)
bwplot(resamps_svm) #SVM
```



# Let's look at the test set performance: comparing ROC MODELS
```{r}
mars.pred = predict(model.mars, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
RF.pred = predict(model.rf, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
LDA.pred = predict(model.lda, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
QDA.pred = predict(model.qda, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
NB.pred = predict(model.nb, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
#SVM.pred = predict(svm.radial.model, newdata = liver_df1[-rowTrain, ]) 
GBM.pred = predict(gbmA.model, newdata = liver_df1[-rowTrain, ], type = "prob")[,1] 
GLM.pred = predict(model.glm, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
GLMN.pred = predict(model.glmn, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
KNN.pred = predict(model.knn, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]

roc.mars <- roc(liver_df1[-rowTrain, ]$outcome, mars.pred)
roc.RF <- roc(liver_df1[-rowTrain, ]$outcome, RF.pred)
roc.LDA <- roc(liver_df1[-rowTrain, ]$outcome, LDA.pred)
roc.QDA <- roc(liver_df1[-rowTrain, ]$outcome, QDA.pred)
roc.NB <- roc(liver_df1[-rowTrain, ]$outcome, NB.pred)
#roc.SVM <- roc(liver_df1[-rowTrain, ]$outcome, SVM.pred)
roc.GBM <- roc(liver_df1[-rowTrain, ]$outcome, GBM.pred)
roc.GLM <- roc(liver_df1[-rowTrain, ]$outcome, GLM.pred)
roc.GLMN <- roc(liver_df1[-rowTrain, ]$outcome, GLMN.pred)
roc.KNN <- roc(liver_df1[-rowTrain, ]$outcome, KNN.pred)


plot(roc.mars, col = 1)
plot(roc.RF, add = T, col = 2)
plot(roc.LDA, add = T, col = 3)
plot(roc.QDA, add = T, col = 4)
plot(roc.NB, add = T, col = 5)
plot(roc.GBM, add = T, col = 6)
plot(roc.GLM, add = T, col = 7)
plot(roc.GLMN, add = T, col = 8)
plot(roc.KNN, add = T, col = 9)

auc <- c(roc.mars$auc[1], roc.RF$auc[1], roc.LDA$auc[1],roc.LDA$auc[1], roc.QDA$auc[1],
         roc.NB$auc[1], roc.GBM$auc[1], roc.GLM$auc[1], roc.GLMN$auc[1],
         roc.KNN$auc[1])
modelNames <- c("Mars","RF", "LDA", "QDA", "NB", "GBM", "GLM", "GLMN", "KNN")

legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)), col = 1:9, lwd = 3, ncol = 2, cex = 0.75)
```

