---
title: "Predicting Diagnosis of Liver Disease "
output: pdf_document
---

```{r}
# Load libraries 
library(tidyverse) 
library(dplyr)
library(ISLR) 
library(janitor) 
library(AppliedPredictiveModeling) 
library(caret) 
library(corrplot) 
library(pROC) 
library(MASS) 
library(readxl)
library(glmnet)
library(mlbench)
library(pdp)
library(vip)
library(klaR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(DALEX)
library(gbm)
```

# Import data
```{r}
liver_df = read_excel("./data/liver.xlsx") %>%
  mutate(outcome = ifelse(is_patient == 1, "yes", "no"), outcome = as.factor(outcome)) %>%
  dplyr::select(-is_patient) %>%
  clean_names %>%
  rename(
    aspartate_aminotransferase = sgpt, 
    alamine_aminotransferase = sgot, 
    albumin_globulin_ratio = ag_ratio, 
    alkaline_phosphate = alkphos) %>%
  drop_na 

liver_df$gender=factor(x=liver_df$gender,levels = c('Female','Male'),labels=c(0, 1))
liver_df$gender = as.double(liver_df$gender) 
# female = '1', male = '2'
```


# Exploratory Data Analysis
```{r}
# Feature plots 
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df[, 1:10], 
            y = liver_df$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# dataset with all the log-transformed predictor variables 
liver_df1 =
  liver_df %>%
  mutate(logtot_bilirubin = log(tot_bilirubin) +1,
         logdirect_bilirubin = log(direct_bilirubin) +1,
         logtot_proteins = log(tot_proteins) +1,
         logalbumin = log(albumin) +1, 
         loggender = log(gender) +1,
         logalbumin_globulin_ratio = log(albumin_globulin_ratio) +1,
         logage = log(age) +1, 
         logaspartate_aminotransferase = log(aspartate_aminotransferase) +1,
         logalamine_transferase = log(alamine_aminotransferase ) +1,
         logalkaline_phosphate = log(alkaline_phosphate) +1) %>% 
         dplyr::select(logage, loggender, logtot_bilirubin, logdirect_bilirubin, logtot_proteins,
                       logalbumin, logalbumin_globulin_ratio, logaspartate_aminotransferase,
                       logalamine_transferase, logalkaline_phosphate, outcome)
  

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df1[, 1:10], 
            y = liver_df1$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# Correlation plot
corrplot(cor(liver_df[,-11]), tl.srt = 45, order = 'hclust', type = 'upper')


table(liver_df$outcome)
```

# Data Partition
```{r}
#Create Training and Test Datasets
set.seed(10)
dim(liver_df1)
rowTrain <- createDataPartition(y = liver_df1$outcome,
                                p = 0.8,
                                list = FALSE)

liver.train = liver_df1[rowTrain, ]
liver.test = liver_df1[-rowTrain, ]
```



# Logistic Regression 
```{r}
#Fit Logistic Regression Model with all predictors 
glm.fit <- glm(outcome ~ ., data = liver_df1,
               subset = rowTrain,
               family = binomial(link = "logit"))
summary(glm.fit)

# Confusion Matrix 
test.pred.prob <- predict(glm.fit, newdata = liver_df1[-rowTrain,], type = "response")
test.pred <- rep("no", length(test.pred.prob)) 
test.pred[test.pred.prob > 0.5] <- "yes"
confusionMatrix(data = as.factor(test.pred), reference = liver_df1$outcome[-rowTrain],
                positive = "yes")
# ROC Curve
roc.glm <- roc(liver_df1$outcome[-rowTrain], test.pred.prob) 
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.glm), col = 4, add = TRUE)

# Fit a logistic regression with CARET 
set.seed(10)
ctrl <- trainControl(method = "repeatedcv", repeats = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
model.glm <- train(x = liver_df1[rowTrain,1:10],
                   y = liver_df1$outcome[rowTrain],
                   method = "glm",
                   preProcess = c("center", "scale"), 
                   metric = "ROC",
                   trControl = ctrl)
```



# Regularized logistic regression
```{r}
ctrl <- trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

set.seed(10)
glmnGrid <- expand.grid(.alpha = seq(0,1,length =6),
                       .lambda = exp(seq(-6,0,length =20)))
model.glmn <- train(x=liver_df1[rowTrain,1:10],
                   y=liver_df1$outcome[rowTrain],
                   method ="glmnet",
                   #preProcess = c("center", "scale"), 
                   tuneGrid =glmnGrid,
                   metric ="ROC",
                   trControl =ctrl)


ggplot(model.glmn,xTrans = function(x)log(x), highlight = TRUE)

max(model.glmn$result$ROC)
model.glmn$bestTune # alpha of 0 indicates a ridge regression. 

coef(model.glmn$finalModel, s = model.glmn$bestTune$lambda)
```


# MARS
```{r}
set.seed(10)
model.mars <- train(x = liver_df1[rowTrain,1:10],
                    y = liver_df1$outcome[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, nprune = 2:20), 
                    metric = "ROC",
                    trControl = ctrl)

plot(model.mars)
coef(model.mars$finalModel)
vip(model.mars$finalModel)
```

# CLASSIFICATION TREE
```{r}
set.seed(1)
tree1 = rpart(formula = outcome ~ ., data = liver_df1, subset = rowTrain, 
              control = rpart.control(cp = 0))
cpTable = printcp(tree1)
minErr = which.min(cpTable[,4])

tree2 = prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
```

# RANDOM FORESTS
```{r}
ctrl2 = trainControl(method = "cv", classProbs = T, summaryFunction = twoClassSummary)

rf.grid = expand.grid(mtry = 1:10, splitrule = "gini", 
                      min.node.size = seq(from = 2, to = 10, by = 2))

set.seed(1)
rf.fit = train(outcome ~ ., liver_df1, subset = rowTrain, method = "ranger", 
               tuneGrid = rf.grid, metric = "ROC", trControl = ctrl2)

ggplot(rf.fit, highlight = T)

rf.pred = predict(rf.fit, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
roc.rf = roc(liver_df1$outcome[-rowTrain], rf.pred)

plot(roc.rf)
auc = c(roc.rf$auc[1])
modelNames = c("Random Forest")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)))
```

# SVM
```{r}
#linear
ctrl <-trainControl(method = "cv")
set.seed(10)
svm.linear.model <- train(
  outcome ~., data = liver.train, method = "svmLinear",
  trControl = ctrl,
  #preProcess = c("center", "scale"),
  tuneGrid =data.frame(C =exp(seq(-10,1,len=20))),
  )

plot(svm.linear.model, highlight = TRUE, xTrans = log)
svm.linear.model$bestTune
svm.linear.model$finalModel
```

```{r}
ctrl <-trainControl(method = "cv")
set.seed(10)
svm.linear2.model <- train(
  outcome ~., 
  data = liver.train, 
  method = "svmLinear2",
  trControl = ctrl,
  #preProcess = c("center", "scale"),
  tuneGrid =data.frame(cost =exp(seq(-10,1,len=20))),
  )

plot(svm.linear2.model, highlight = TRUE, xTrans = log)
svm.linear2.model$bestTune
svm.linear2.model$finalModel
```

```{r}
svmr.grid <- expand.grid(C = exp(seq(-10,1,len=20)),
                         sigma = exp(seq(-8,0,len=10)))
# tunes over both cost and sigma
set.seed(10)
svm.radial.model <- train(outcome ~., 
                          data = liver.train, 
                          method = "svmRadial",
                          trControl = ctrl,
                          preProcess = c("center", "scale"),
                          tuneGrid = svmr.grid,
                          prob.model = TRUE,
                          verbose=F)

plot(svm.radial.model, highlight = TRUE)
svm.radial.model$bestTune
svm.radial.model$finalModel
```

```{r}
x_train <- as.matrix(liver.train[,1:10])

explainer_svm <- explain(svm.radial.model,
                         label = "svmr",
                         data = x_train,
                         y = as.numeric(liver_df1$outcome[rowTrain] == "yes"),
                         verbose = FALSE)
vi_svm <- model_parts(explainer_svm)
plot(vi_svm)
```

# Boosting
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

ctrl.boost <- trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
set.seed(10)
gbmA.model <- train(outcome ~ . ,
                  data = liver.train,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl.boost,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.model, highlight = TRUE)

gbmImp <- varImp(gbmA.model, scale = FALSE)
plot(gbmImp)
```

