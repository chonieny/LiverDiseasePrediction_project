---
title: "Predicting Diagnosis of Liver Disease "
output: pdf_document
---

```{r}
# Load libraries 
library(tidyverse) 
library(dplyr)
library(ISLR) 
library(janitor) 
library(AppliedPredictiveModeling) 
library(caret) 
library(corrplot) 
library(pROC) 
library(MASS) 
library(readxl)
library(glmnet)
library(mlbench)
library(pdp)
library(vip)
library(klaR)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(ISLR)
library(caret)
library(e1071)
library(kernlab)
library(DALEX)
library(gbm)
library(ROCR)

```

# Import data
```{r}
liver_df = read_excel("./data/liver.xlsx") %>%
  mutate(outcome = ifelse(is_patient == 1, "yes", "no"), outcome = as.factor(outcome)) %>%
  dplyr::select(-is_patient) %>%
  clean_names %>%
  rename(
    aspartate_aminotransferase = sgpt, 
    alamine_aminotransferase = sgot, 
    albumin_globulin_ratio = ag_ratio, 
    alkaline_phosphate = alkphos) %>%
  drop_na 

liver_df$gender=factor(x=liver_df$gender,levels = c('Female','Male'),labels=c(0, 1))
liver_df$gender = as.double(liver_df$gender) 
# female = '1', male = '2'
```


# Exploratory Data Analysis
```{r}
# Feature plots 
theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df[, 1:10], 
            y = liver_df$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# dataset with all the log-transformed predictor variables 
liver_df1 =
  liver_df %>%
  mutate(logtot_bilirubin = log(tot_bilirubin) +1,
         logdirect_bilirubin = log(direct_bilirubin) +1,
         logtot_proteins = log(tot_proteins) +1,
         logalbumin = log(albumin) +1, 
         loggender = log(gender) +1,
         logalbumin_globulin_ratio = log(albumin_globulin_ratio) +1,
         logage = log(age) +1, 
         logaspartate_aminotransferase = log(aspartate_aminotransferase) +1,
         logalamine_transferase = log(alamine_aminotransferase ) +1,
         logalkaline_phosphate = log(alkaline_phosphate) +1) %>% 
         dplyr::select(logage, loggender, logtot_bilirubin, logdirect_bilirubin, logtot_proteins,
                       logalbumin, logalbumin_globulin_ratio, logaspartate_aminotransferase,
                       logalamine_transferase, logalkaline_phosphate, outcome)
  

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = liver_df1[, 1:10], 
            y = liver_df1$outcome,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# Correlation plot
corrplot(cor(liver_df[,-11]), tl.srt = 45, order = 'hclust', type = 'upper')


table(liver_df$outcome)
```

# Data Partition
```{r}
#Create Training and Test Datasets
set.seed(10)
dim(liver_df1)
rowTrain <- createDataPartition(y = liver_df1$outcome,
                                p = 0.8,
                                list = FALSE)

liver.train = liver_df1[rowTrain, ]
liver.test = liver_df1[-rowTrain, ]
```



# Logistic Regression 
```{r}
#Fit Logistic Regression Model with all predictors 
glm.fit <- glm(outcome ~ ., data = liver_df1,
               subset = rowTrain,
               family = binomial(link = "logit"))
summary(glm.fit)

# Confusion Matrix 
test.pred.prob <- predict(glm.fit, newdata = liver_df1[-rowTrain,], type = "response")
test.pred <- rep("no", length(test.pred.prob)) 
test.pred[test.pred.prob > 0.5] <- "yes"
confusionMatrix(data = as.factor(test.pred), reference = liver_df1$outcome[-rowTrain],
                positive = "yes")
# ROC Curve
roc.glm <- roc(liver_df1$outcome[-rowTrain], test.pred.prob) 
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE) 
plot(smooth(roc.glm), col = 4, add = TRUE)

# Fit a logistic regression with CARET 
set.seed(10)
ctrl1 <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
model.glm <- train(x = liver_df1[rowTrain,1:10],
                   y = liver_df1$outcome[rowTrain],
                   method = "glm",
                   preProcess = c("center", "scale"), 
                   metric = "ROC",
                   trControl = ctrl1)
```



# Regularized logistic regression
```{r}
ctrl2 <- trainControl(method = "cv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

set.seed(10)
glmnGrid <- expand.grid(.alpha = seq(0,1,length =6),
                       .lambda = exp(seq(-6,0,length =20)))
model.glmn <- train(x=liver_df1[rowTrain,1:10],
                   y=liver_df1$outcome[rowTrain],
                   method ="glmnet",
                   #preProcess = c("center", "scale"), 
                   tuneGrid =glmnGrid,
                   metric ="ROC",
                   trControl =ctrl2)


ggplot(model.glmn,xTrans = function(x)log(x), highlight = TRUE)

max(model.glmn$result$ROC)
model.glmn$bestTune # alpha of 0 indicates a ridge regression. 

coef(model.glmn$finalModel, s = model.glmn$bestTune$lambda)
```


# MARS
```{r}
set.seed(10)
model.mars <- train(x = liver_df1[rowTrain,1:10],
                    y = liver_df1$outcome[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, nprune = 2:20), 
                    metric = "ROC",
                    trControl = ctrl2)

plot(model.mars)
coef(model.mars$finalModel)
vip(model.mars$finalModel)
```
# KNN
```{r}
set.seed(10)

model.knn <- train(x = liver_df1[rowTrain,1:10],
                   y = liver_df1$outcome[rowTrain],
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1, 200, by = 5)),  
                   trControl = ctrl1)

liver_df1$outcome[-rowTrain]
model.knn$bestTune
pred_knn = predict(model.knn, newdata = liver.test, type = 'prob')
roc_knn <- roc(liver_df1$outcome[-rowTrain], pred_knn[,2])
plot.roc(roc_knn, legacy.axes = TRUE, print.auc = TRUE)
```

# LDA
```{r}
set.seed(10)
lda.fit <- lda(outcome~., 
               data = liver.train)

set.seed(10)
model.lda <- train(x = liver_df1[rowTrain,1:10], 
                   y = liver_df1$outcome[rowTrain],
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl1)
lda.pred <- predict(lda.fit, newdata = liver.test)
roc.lda <- roc(liver_df1[-rowTrain,]$outcome, lda.pred$posterior[,2])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```

# QDA
```{r}
set.seed(10)
qda.fit <- qda(outcome~., 
               data = liver.train)

set.seed(10)
model.qda <- train(x = liver_df1[rowTrain,1:10], 
                   y = liver_df1$outcome[rowTrain],
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl1)
qda.pred <- predict(qda.fit, newdata = liver.test)
roc.qda <- roc(liver_df1[-rowTrain,]$outcome, qda.pred$posterior[,2])
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)


auc <- c(roc.lda$auc[1], roc.qda$auc[1])
```


# Naive Bayes
```{r}
nbGrid <- expand.grid(usekernel = c(FALSE,TRUE), 
                      fL = 1,
                      adjust = seq(.2, 3, by = .2))
model.nb <- train(x = liver_df1[rowTrain,1:10], 
                  y = liver_df1$outcome[rowTrain],
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl1)
plot(model.nb)
```



# CLASSIFICATION TREE
```{r}
set.seed(1)
tree1 = rpart(formula = outcome ~ ., data = liver_df1, subset = rowTrain, 
              control = rpart.control(cp = 0))
rpart.plot(tree1)
#summary(tree1)
```

# RANDOM FORESTS
```{r}


#rf.grid = expand.grid(mtry = 1:10, splitrule = "gini", 
                      #min.node.size = seq(from = 2, to = 10, by = 2))

#set.seed(1)
#model.rf = train(outcome ~ ., liver_df1, subset = rowTrain, method = "ranger", 
               #tuneGrid = rf.grid, metric = "ROC", trControl = ctrl1)

#RF.pred = predict(model.rf, newdata = liver_df1[-rowTrain, ], type = "prob")
#roc.rf = roc(liver_df1$outcome[-rowTrain], RF.pred) #error appears for this line 
#auc.rf = c(roc.rf$auc[1])
#auc.rf
```

# SVM
```{r}
#linear
ctrl4 <-trainControl(method = "cv")
set.seed(10)
svm.linear.model <- train(
  outcome ~., data = liver.train, method = "svmLinear",
  trControl = ctrl4,
  #preProcess = c("center", "scale"),
  tuneGrid =data.frame(C =exp(seq(-10,1,len=20))),
  )

plot(svm.linear.model, highlight = TRUE, xTrans = log)
svm.linear.model$bestTune
svm.linear.model$finalModel
```

```{r}
ctrl4 <-trainControl(method = "cv")
set.seed(10)
svm.linear2.model <- train(
  outcome ~., 
  data = liver.train, 
  method = "svmLinear2",
  trControl = ctrl4,
  #preProcess = c("center", "scale"),
  tuneGrid =data.frame(cost =exp(seq(-10,1,len=20))),
  )

plot(svm.linear2.model, highlight = TRUE, xTrans = log)
svm.linear2.model$bestTune
svm.linear2.model$finalModel
```

```{r}
svmr.grid <- expand.grid(C = exp(seq(-1, 4,len = 10)), sigma = exp(seq(-10,0,len = 10)))

# tunes over both cost and sigma
set.seed(10)

svm.radial.model <- train(outcome ~ . , liver_df1, 
                  subset = rowTrain,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  trControl = ctrl4)

plot(svm.radial.model, highlight = TRUE)
svm.radial.model$bestTune
svm.radial.model$finalModel
```

```{r}
x_train <- as.matrix(liver.train[,1:10])

explainer_svm <- explain(svm.radial.model,
                         label = "svmr",
                         data = x_train,
                         y = as.numeric(liver_df1$outcome[rowTrain] == "yes"),
                         verbose = FALSE)
vi_svm <- model_parts(explainer_svm)
plot(vi_svm)
```

# Boosting
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

ctrl.boost <- trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)
set.seed(10)
gbmA.model <- train(outcome ~ . ,
                  data = liver.train,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl.boost,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.model, highlight = TRUE)

gbmImp <- varImp(gbmA.model, scale = FALSE)
plot(gbmImp)
```

# Let's select the Final Model by comparing each model's mean cross-validation AUC 
```{r}
#res <- resamples(list(MARS = model.mars, RF = model.rf, SVM = svm.radial.model, GBM = gbmA.model, LDA = model.lda, QDA = model.qda, NB = model.nb, GLM = model.glm, KNN = model.knn))
#summary(res)
# bwplot(res, metric = "ROC")  #This line doesn't run 
```



# Let's look at the test set performance: comparing ROC MODELS
```{r}
mars.pred = predict(model.mars, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
#RF.pred = predict(model.rf, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
LDA.pred = predict(model.lda, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
QDA.pred = predict(model.qda, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
NB.pred = predict(model.nb, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
SVM.pred = predict(svm.radial.model, newdata = liver_df1[-rowTrain, ]) 
GBM.pred = predict(gbmA.model, newdata = liver_df1[-rowTrain, ], type = "prob")[,1] 
GLM.pred = predict(model.glm, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
GLMN.pred = predict(model.glmn, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]
KNN.pred = predict(model.knn, newdata = liver_df1[-rowTrain, ], type = "prob")[,1]

pred1 = prediction(as.numeric(mars.pred), liver_df1$outcome[-rowTrain])
perf1 = performance(pred1, "tpr", "fpr")

#pred2 = prediction(as.numeric(RF.pred), liver_df1$outcome[-rowTrain])
#perf2 = performance(pred2, "tpr", "fpr")

pred3 = prediction(as.numeric(LDA.pred), liver_df1$outcome[-rowTrain])
perf3 = performance(pred3, "tpr", "fpr")

pred4 = prediction(as.numeric(QDA.pred), liver_df1$outcome[-rowTrain])
perf4 = performance(pred4, "tpr", "fpr")

pred5 = prediction(as.numeric(NB.pred), liver_df1$outcome[-rowTrain])
perf5 = performance(pred5, "tpr", "fpr")

pred6 = prediction(as.numeric(SVM.pred), liver_df1$outcome[-rowTrain]) 
perf6 = performance(pred6, "tpr", "fpr")

pred7 = prediction(as.numeric(GBM.pred), liver_df1$outcome[-rowTrain])
perf7 = performance(pred7, "tpr", "fpr")

pred8 = prediction(as.numeric(GLM.pred), liver_df1$outcome[-rowTrain])
perf8 = performance(pred8, "tpr", "fpr")

pred9 = prediction(as.numeric(GLMN.pred), liver_df1$outcome[-rowTrain])
perf9 = performance(pred9, "tpr", "fpr")

pred10 = prediction(as.numeric(KNN.pred), liver_df1$outcome[-rowTrain])
perf10 = performance(pred10, "tpr", "fpr")


plot(perf1, col = "red"); par(new = T)
#plot(perf2, col = "blue"); par(new = T)
plot(perf3, col = "green"); par(new = T)
plot(perf4, col = "orange"); par(new = T)
plot(perf5, col = "yellow"); par(new = T)
plot(perf6, col = "purple"); par(new = T)
plot(perf7, col = "pink"); par(new = T)
plot(perf8, col = "brown"); par(new = T)
plot(perf9, col = "turquoise"); par(new = T)
plot(perf10, col = "navy")

#legend("topleft", c("Mars","RF SVM", "LDA", "QDA", "NB", "SVM", "GBM", "GLM", "GLMN", "KNN"),
       #lty = c(1,1), lwd = c(2.5,2.5), 
       #col = c("red", "blue", "green", "orange", "yellow", "purple", "pink", "black", "brown",
             #"turquoise", "navy"), bty = "n")
```

